# ResNet Float Precision ëª¨ë‹ˆí„°ë§ - Colab ì‚¬ìš© ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸
```python
# Colabì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip install seaborn matplotlib pandas

# ì½”ë“œ ì‹¤í–‰
exec(open('resnet_precision_monitor.py').read())
exec(open('float_bit_analyzer.py').read())
```

### 2. ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘
```python
# ëª¨ë‹ˆí„°ë§ ì‹œì‘
start_monitoring()

# í¬ê´„ì ì¸ ë¶„ì„ ì‹¤í–‰
run_comprehensive_analysis()
```

## ğŸ“Š ì£¼ìš” ê¸°ëŠ¥ë³„ ì‚¬ìš©ë²•

### A. ResNet ë‹¤ì¤‘ ì •ë°€ë„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§

```python
# 1. íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
trainer = MultiPrecisionTrainer(model_name='resnet18', num_classes=10)

# 2. ë°ì´í„° ì¤€ë¹„
trainloader, testloader = trainer.prepare_data(batch_size=32)

# 3. ì—¬ëŸ¬ ì •ë°€ë„ë¡œ í›ˆë ¨ í…ŒìŠ¤íŠ¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

for precision in ['fp32', 'fp16']:
    print(f"Testing {precision}")
    
    model = trainer.create_model(precision).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    
    # í•œ ì—í­ í›ˆë ¨ (ëª¨ë‹ˆí„°ë§ í¬í•¨)
    avg_loss = trainer.train_epoch(
        model, trainloader, criterion, optimizer,
        precision, device, monitor_frequency=5
    )
    
    print(f"{precision} average loss: {avg_loss:.6f}")

# 4. ê²°ê³¼ ë¶„ì„
trainer.analyze_precision_comparison()
trainer.plot_precision_analysis()
trainer.generate_detailed_report()
```

### B. ë¹„íŠ¸ ë ˆë²¨ ë¶„ì„

```python
# 1. ë¶„ì„ê¸° ì´ˆê¸°í™”
analyzer = FloatBitAnalyzer()

# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
test_data = torch.randn(1000) * 10000  # í° ë²”ìœ„ì˜ ëœë¤ ë°ì´í„°

# 3. FP32ì™€ FP16 ë¹„êµ ë¶„ì„
fp32_tensor = test_data.float()
fp16_tensor = test_data.half()

fp32_analysis = analyzer.analyze_tensor_bits(fp32_tensor, 'fp32')
fp16_analysis = analyzer.analyze_tensor_bits(fp16_tensor, 'fp16')

print("FP32 Analysis:", fp32_analysis)
print("FP16 Analysis:", fp16_analysis)

# 4. ê°œë³„ ê°’ì˜ ë¹„íŠ¸ íŒ¨í„´ í™•ì¸
sample_value = 12345.67
print("FP32 bits:", analyzer.float32_to_bits(sample_value))
print("FP16 bits:", analyzer.float16_to_bits(sample_value))
```

### C. NaN ë°œìƒ ì›ì¸ ì¡°ì‚¬

```python
# 1. NaN ì¡°ì‚¬ê¸° ì´ˆê¸°í™”
investigator = NaNInvestigator()

# 2. ë¬¸ì œê°€ ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
investigator.analyze_scenarios()

# 3. ì»¤ìŠ¤í…€ ì—°ì‚° ëª¨ë‹ˆí„°ë§
a = torch.tensor([1e20, 1e25], dtype=torch.float16)
b = torch.tensor([1e15, 1e20], dtype=torch.float16)
result = investigator.monitor_operations(a, b, 'multiplication', a * b)

# 4. NaN ì†ŒìŠ¤ í™•ì¸
print("NaN sources found:", len(investigator.nan_sources))
for source in investigator.nan_sources:
    print(f"Operation: {source['operation']}")
    print(f"Result stats: {source['result_stats']}")
```

### D. ì •ë°€ë„ ë³€í™˜ í…ŒìŠ¤íŠ¸

```python
# 1. ë²”ìœ„ íƒêµ¬ê¸° ì´ˆê¸°í™”
explorer = PrecisionRangeExplorer()

# 2. ì •ë°€ë„ ë³€í™˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
conversion_results = explorer.test_precision_conversion()

# 3. ê²°ê³¼ ë¶„ì„
print("Conversion test results:")
print(conversion_results.describe())

# 4. ì‹œê°í™”
explorer.plot_precision_analysis(conversion_results)

# 5. ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜ ë¶„ì„ (ëª¨ë¸ í›ˆë ¨ í›„)
# model = your_trained_model
# overflow_risks = explorer.analyze_gradient_overflow_risk(model)
# print("Overflow risks:", overflow_risks)
```

## ğŸ” íŠ¹ì • ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 1. FP16ì—ì„œ Lossê°€ NaNì´ ë˜ëŠ” ê²½ìš°

```python
# ë¬¸ì œ ì§„ë‹¨
def diagnose_fp16_nan_loss():
    # ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸
    model = models.resnet18(num_classes=10).half().cuda()
    criterion = nn.CrossEntropyLoss()
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    dummy_input = torch.randn(4, 3, 32, 32).half().cuda()
    dummy_target = torch.randint(0, 10, (4,)).cuda()
    
    # Forward pass ëª¨ë‹ˆí„°ë§
    output = model(dummy_input)
    loss = criterion(output, dummy_target)
    
    print(f"Output stats: min={output.min()}, max={output.max()}, has_nan={torch.isnan(output).any()}")
    print(f"Loss: {loss.item()}, is_nan={torch.isnan(loss)}")
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ëª¨ë‹ˆí„°ë§
    loss.backward()
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = torch.norm(param.grad)
            if torch.isnan(grad_norm) or grad_norm > 1000:
                print(f"Problematic gradient in {name}: norm={grad_norm}")

diagnose_fp16_nan_loss()
```

### 2. ì»¤ìŠ¤í…€ float í¬ë§· ì‹¤í—˜

```python
# ì»¤ìŠ¤í…€ í¬ë§· ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
create_custom_float_format_simulator()

# íŠ¹ì • ì§€ìˆ˜/ê°€ìˆ˜ ë¹„íŠ¸ ì¡°í•© í…ŒìŠ¤íŠ¸
custom_format = CustomFloatFormat(sign_bits=1, exponent_bits=6, mantissa_bits=17)
limits = custom_format.get_limits()
print("Custom format limits:", limits)
```

### 3. ì‹¤ì‹œê°„ í›ˆë ¨ ëª¨ë‹ˆí„°ë§

```python
class RealTimeMonitor:
    def __init__(self):
        self.loss_history = []
        self.nan_detected = False
    
    def monitor_batch(self, batch_idx, loss, model):
        # Loss ê¸°ë¡
        self.loss_history.append(float(loss.item()))
        
        # NaN ì²´í¬
        if torch.isnan(loss):
            self.nan_detected = True
            print(f"ğŸš¨ NaN detected at batch {batch_idx}!")
            
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì§„ë‹¨
            for name, param in model.named_parameters():
                if torch.isnan(param).any():
                    print(f"NaN in parameter: {name}")
                if param.grad is not None and torch.isnan(param.grad).any():
                    print(f"NaN in gradient: {name}")
        
        # ì£¼ê¸°ì  ë¦¬í¬íŠ¸
        if batch_idx % 10 == 0:
            recent_losses = self.loss_history[-10:]
            avg_loss = sum(recent_losses) / len(recent_losses)
            print(f"Batch {batch_idx}: Avg Loss = {avg_loss:.6f}")

# ì‚¬ìš© ì˜ˆì œ
monitor = RealTimeMonitor()
# í›ˆë ¨ ë£¨í”„ì—ì„œ monitor.monitor_batch(batch_idx, loss, model) í˜¸ì¶œ
```

## ğŸ“ˆ ê²°ê³¼ í•´ì„ ê°€ì´ë“œ

### Loss NaNì˜ ì£¼ìš” ì›ì¸ë“¤:

1. **ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ**: ë§¤ìš° í° ê·¸ë˜ë””ì–¸íŠ¸ ê°’
2. **Learning rate ê³¼ë‹¤**: ì—…ë°ì´íŠ¸ ìŠ¤í…ì´ ë„ˆë¬´ í¼
3. **ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±**: FP16ì˜ ì œí•œëœ ë²”ìœ„
4. **ë°°ì¹˜ ì •ê·œí™” ë¬¸ì œ**: ì‘ì€ ë¶„ì‚°ìœ¼ë¡œ ì¸í•œ ë‚˜ëˆ—ì…ˆ ì˜¤ë¥˜

### í•´ê²° ë°©ë²•:

1. **Gradient Clipping ì ìš©**
2. **Learning Rate ê°ì†Œ**
3. **Mixed Precision í›ˆë ¨ ì‚¬ìš©**
4. **Loss Scaling ì ìš©**

```python
# ì•ˆì •ì ì¸ FP16 í›ˆë ¨ì„ ìœ„í•œ ì„¤ì •
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# í›ˆë ¨ ë£¨í”„ì—ì„œ
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## ğŸ¯ ì—°êµ¬ ë°©í–¥ ì œì•ˆ

1. **ì§€ìˆ˜/ê°€ìˆ˜ ë¹„íŠ¸ ë¹„ìœ¨ ìµœì í™”**: íŠ¹ì • ë„ë©”ì¸ì— ìµœì í™”ëœ í¬ë§· ì°¾ê¸°
2. **ì ì‘ì  ì •ë°€ë„**: í›ˆë ¨ ê³¼ì •ì—ì„œ ë™ì ìœ¼ë¡œ ì •ë°€ë„ ì¡°ì ˆ
3. **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**: ë ˆì´ì–´ë³„ë¡œ ë‹¤ë¥¸ ì •ë°€ë„ ì‚¬ìš©
4. **ì •ë°€ë„ ì¸ì‹ ìµœì í™”**: ì •ë°€ë„ ì œì•½ì„ ê³ ë ¤í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜

ì´ ë„êµ¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ FP16 í›ˆë ¨ì˜ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!