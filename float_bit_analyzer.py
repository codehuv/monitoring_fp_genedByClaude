import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import struct
from collections import Counter, defaultdict
import pandas as pd
from IPython.display import display, HTML

class FloatBitAnalyzer:
    """Floatì˜ ë¹„íŠ¸ ìˆ˜ì¤€ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    @staticmethod
    def float32_to_bits(f):
        """Float32ë¥¼ ë¹„íŠ¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
        bits = struct.unpack('>I', struct.pack('>f', f))[0]
        return format(bits, '032b')
    
    @staticmethod
    def float16_to_bits(f):
        """Float16ì„ ë¹„íŠ¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
        # PyTorch tensorë¥¼ í†µí•´ ë³€í™˜
        tensor = torch.tensor(f, dtype=torch.float16)
        bits = tensor.view(torch.int16).item()
        if bits < 0:
            bits = bits + 2**16
        return format(bits, '016b')
    
    @staticmethod
    def parse_float32_bits(bits_str):
        """Float32 ë¹„íŠ¸ ë¬¸ìì—´ì„ sign, exponent, mantissaë¡œ ë¶„í•´"""
        sign = int(bits_str[0])
        exponent = int(bits_str[1:9], 2)
        mantissa = int(bits_str[9:], 2)
        return sign, exponent, mantissa
    
    @staticmethod
    def parse_float16_bits(bits_str):
        """Float16 ë¹„íŠ¸ ë¬¸ìì—´ì„ sign, exponent, mantissaë¡œ ë¶„í•´"""
        sign = int(bits_str[0])
        exponent = int(bits_str[1:6], 2)
        mantissa = int(bits_str[6:], 2)
        return sign, exponent, mantissa
    
    @classmethod
    def analyze_tensor_bits(cls, tensor, precision='fp32'):
        """í…ì„œì˜ ëª¨ë“  ê°’ì— ëŒ€í•œ ë¹„íŠ¸ ìˆ˜ì¤€ ë¶„ì„"""
        if tensor.numel() == 0:
            return {}
        
        # ìœ í•œí•œ ê°’ë§Œ ë¶„ì„
        finite_tensor = tensor[torch.isfinite(tensor)]
        if finite_tensor.numel() == 0:
            return {'error': 'No finite values to analyze'}
        
        values = finite_tensor.cpu().numpy().flatten()
        
        bit_analysis = {
            'total_values': len(values),
            'exponent_dist': Counter(),
            'mantissa_patterns': Counter(),
            'sign_dist': Counter(),
            'zero_mantissa_count': 0,
            'denormal_count': 0,
            'special_values': {'nan': 0, 'inf': 0, 'zero': 0}
        }
        
        for val in values:
            if np.isnan(val):
                bit_analysis['special_values']['nan'] += 1
                continue
            elif np.isinf(val):
                bit_analysis['special_values']['inf'] += 1
                continue
            elif val == 0.0:
                bit_analysis['special_values']['zero'] += 1
                continue
            
            if precision == 'fp32':
                bits = cls.float32_to_bits(float(val))
                sign, exponent, mantissa = cls.parse_float32_bits(bits)
                bias = 127
            else:  # fp16
                bits = cls.float16_to_bits(float(val))
                sign, exponent, mantissa = cls.parse_float16_bits(bits)
                bias = 15
            
            bit_analysis['sign_dist'][sign] += 1
            bit_analysis['exponent_dist'][exponent] += 1
            
            if mantissa == 0:
                bit_analysis['zero_mantissa_count'] += 1
            
            # Denormal (subnormal) ìˆ˜ í™•ì¸
            if exponent == 0 and mantissa != 0:
                bit_analysis['denormal_count'] += 1
            
            # ì§€ìˆ˜ì˜ ì‹¤ì œ ê°’ (bias ì œê±°)
            if exponent != 0:
                actual_exp = exponent - bias
                bit_analysis['mantissa_patterns'][f'exp_{actual_exp}'] += 1
        
        return bit_analysis

class PrecisionRangeExplorer:
    """Float ì •ë°€ë„ì˜ ë²”ìœ„ì™€ í•œê³„ë¥¼ íƒêµ¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_values = self._generate_test_values()
    
    def _generate_test_values(self):
        """ë‹¤ì–‘í•œ ë²”ìœ„ì˜ í…ŒìŠ¤íŠ¸ ê°’ ìƒì„±"""
        values = []
        
        # ì •ìƒì ì¸ ë²”ìœ„ì˜ ê°’ë“¤
        values.extend(np.logspace(-10, 10, 100))
        values.extend(-np.logspace(-10, 10, 100))
        
        # ê·¹í•œê°’ë“¤
        values.extend([1e-45, 1e-40, 1e-35, 1e-30])  # ë§¤ìš° ì‘ì€ ê°’
        values.extend([1e30, 1e35, 1e38, 1e39])      # ë§¤ìš° í° ê°’
        
        # ê²½ê³„ê°’ë“¤
        values.extend([65504, 65505, 65520])  # FP16 í•œê³„ ê·¼ì²˜
        values.extend([0.0, -0.0])            # Zero values
        values.extend([float('inf'), float('-inf'), float('nan')])
        
        return np.array(values, dtype=np.float32)
    
    def test_precision_conversion(self):
        """ì •ë°€ë„ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
        results = {
            'original': [],
            'fp32': [],
            'fp16': [],
            'fp32_recovered': [],
            'conversion_error': [],
            'precision_loss': []
        }
        
        for val in self.test_values:
            if not np.isfinite(val):
                continue
                
            original = float(val)
            
            # FP32 ë³€í™˜
            fp32_tensor = torch.tensor(original, dtype=torch.float32)
            fp32_val = float(fp32_tensor)
            
            # FP16 ë³€í™˜
            try:
                fp16_tensor = torch.tensor(original, dtype=torch.float16)
                fp16_val = float(fp16_tensor)
                
                # FP16ì—ì„œ ë‹¤ì‹œ FP32ë¡œ
                fp32_recovered = float(fp16_tensor.float())
                
                conversion_error = abs(original - fp16_val)
                precision_loss = abs(fp32_val - fp32_recovered)
                
                results['original'].append(original)
                results['fp32'].append(fp32_val)
                results['fp16'].append(fp16_val)
                results['fp32_recovered'].append(fp32_recovered)
                results['conversion_error'].append(conversion_error)
                results['precision_loss'].append(precision_loss)
                
            except (RuntimeError, OverflowError):
                # FP16 ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê²½ìš°
                continue
        
        return pd.DataFrame(results)
    
    def analyze_gradient_overflow_risk(self, model):
        """ëª¨ë¸ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜ ë¶„ì„"""
        overflow_risks = {}
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad = param.grad.data
                
                # í†µê³„ ê³„ì‚°
                risk_factors = {
                    'max_abs_grad': float(torch.max(torch.abs(grad))),
                    'grad_norm': float(torch.norm(grad)),
                    'large_grad_ratio': float((torch.abs(grad) > 1000).sum()) / grad.numel(),
                    'small_grad_ratio': float((torch.abs(grad) < 1e-6).sum()) / grad.numel(),
                    'fp16_overflow_risk': float(torch.max(torch.abs(grad))) > 65000
                }
                
                overflow_risks[name] = risk_factors
        
        return overflow_risks
    
    def plot_precision_analysis(self, df):
        """ì •ë°€ë„ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Float Precision Analysis', fontsize=16)
        
        # 1. ì›ë³¸ vs FP16 ê°’ ë¹„êµ
        ax1 = axes[0, 0]
        valid_idx = (np.isfinite(df['original']) & np.isfinite(df['fp16']))
        ax1.scatter(df['original'][valid_idx], df['fp16'][valid_idx], alpha=0.6, s=1)
        ax1.plot([-1e10, 1e10], [-1e10, 1e10], 'r--', alpha=0.7)
        ax1.set_xlabel('Original (FP32)')
        ax1.set_ylabel('FP16')
        ax1.set_title('FP32 vs FP16 Values')
        ax1.set_xscale('symlog')
        ax1.set_yscale('symlog')
        
        # 2. ë³€í™˜ ì˜¤ì°¨ ë¶„í¬
        ax2 = axes[0, 1]
        valid_errors = df['conversion_error'][df['conversion_error'] > 0]
        if len(valid_errors) > 0:
            ax2.hist(np.log10(valid_errors), bins=50, alpha=0.7)
            ax2.set_xlabel('Log10(Conversion Error)')
            ax2.set_ylabel('Count')
            ax2.set_title('Conversion Error Distribution')
        
        # 3. ì •ë°€ë„ ì†ì‹¤ ë¶„í¬
        ax3 = axes[0, 2]
        valid_loss = df['precision_loss'][df['precision_loss'] > 0]
        if len(valid_loss) > 0:
            ax3.hist(np.log10(valid_loss), bins=50, alpha=0.7)
            ax3.set_xlabel('Log10(Precision Loss)')
            ax3.set_ylabel('Count')
            ax3.set_title('Precision Loss Distribution')
        
        # 4. ê°’ì˜ í¬ê¸°ë³„ ì˜¤ì°¨ìœ¨
        ax4 = axes[1, 0]
        abs_original = np.abs(df['original'])
        relative_error = df['conversion_error'] / (abs_original + 1e-45)
        valid_idx = np.isfinite(relative_error) & (abs_original > 0)
        
        if np.sum(valid_idx) > 0:
            ax4.scatter(abs_original[valid_idx], relative_error[valid_idx], alpha=0.6, s=1)
            ax4.set_xlabel('Absolute Original Value')
            ax4.set_ylabel('Relative Error')
            ax4.set_title('Relative Error vs Value Magnitude')
            ax4.set_xscale('log')
            ax4.set_yscale('log')
        
        # 5. FP16 ë²”ìœ„ í•œê³„ ì‹œê°í™”
        ax5 = axes[1, 1]
        fp16_max = 65504
        fp16_min = 6.103515625e-05
        
        original_abs = np.abs(df['original'])
        in_range = (original_abs <= fp16_max) & (original_abs >= fp16_min)
        out_range = ~in_range & np.isfinite(df['original'])
        
        ax5.scatter(df['original'][in_range], df['fp16'][in_range], 
                   alpha=0.6, s=1, label='In FP16 range', color='green')
        ax5.scatter(df['original'][out_range], df['fp16'][out_range], 
                   alpha=0.6, s=1, label='Out of FP16 range', color='red')
        ax5.axhline(y=fp16_max, color='red', linestyle='--', alpha=0.7, label='FP16 max')
        ax5.axhline(y=-fp16_max, color='red', linestyle='--', alpha=0.7)
        ax5.set_xlabel('Original Value')
        ax5.set_ylabel('FP16 Value')
        ax5.set_title('FP16 Range Limitations')
        ax5.legend()
        ax5.set_yscale('symlog')
        ax5.set_xscale('symlog')
        
        # 6. ì§€ìˆ˜ ë¶„í¬ ë¹„êµ
        ax6 = axes[1, 2]
        
        # FP32ì™€ FP16ì˜ ì§€ìˆ˜ ë²”ìœ„ ë¹„êµ
        fp32_exp_range = range(-126, 128)  # FP32 ì§€ìˆ˜ ë²”ìœ„
        fp16_exp_range = range(-14, 16)    # FP16 ì§€ìˆ˜ ë²”ìœ„
        
        ax6.bar([f'FP32\n({min(fp32_exp_range)} to {max(fp32_exp_range)})'], 
               [len(fp32_exp_range)], alpha=0.7, label='FP32 Exponent Range')
        ax6.bar([f'FP16\n({min(fp16_exp_range)} to {max(fp16_exp_range)})'], 
               [len(fp16_exp_range)], alpha=0.7, label='FP16 Exponent Range')
        ax6.set_ylabel('Number of Exponent Values')
        ax6.set_title('Exponent Range Comparison')
        ax6.legend()
        
        plt.tight_layout()
        plt.show()

class NaNInvestigator:
    """NaN ë°œìƒ ì›ì¸ì„ ì¡°ì‚¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.nan_sources = []
        self.operation_log = []
    
    def monitor_operations(self, tensor1, tensor2, operation, result):
        """ì—°ì‚° ëª¨ë‹ˆí„°ë§"""
        log_entry = {
            'operation': operation,
            'input1_stats': self._get_tensor_stats(tensor1),
            'input2_stats': self._get_tensor_stats(tensor2) if tensor2 is not None else None,
            'result_stats': self._get_tensor_stats(result),
            'nan_introduced': torch.isnan(result).sum().item() > 0
        }
        
        if log_entry['nan_introduced']:
            self.nan_sources.append(log_entry)
        
        self.operation_log.append(log_entry)
        return result
    
    def _get_tensor_stats(self, tensor):
        """í…ì„œ í†µê³„ ê³„ì‚°"""
        if tensor is None:
            return None
        
        return {
            'shape': tuple(tensor.shape),
            'dtype': str(tensor.dtype),
            'min': float(tensor.min()) if tensor.numel() > 0 else None,
            'max': float(tensor.max()) if tensor.numel() > 0 else None,
            'mean': float(tensor.mean()) if tensor.numel() > 0 else None,
            'nan_count': int(torch.isnan(tensor).sum()),
            'inf_count': int(torch.isinf(tensor).sum()),
            'very_large_count': int((torch.abs(tensor) > 1e10).sum()),
            'very_small_count': int((torch.abs(tensor) < 1e-10).sum())
        }
    
    def create_problematic_scenarios(self):
        """ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë° í…ŒìŠ¤íŠ¸"""
        scenarios = []
        
        # 1. ë§¤ìš° í° ê°’ë“¤ì˜ ê³±ì…ˆ
        large_vals = torch.tensor([1e20, 1e25, 1e30], dtype=torch.float16)
        result1 = large_vals * large_vals
        scenarios.append(('Large multiplication', large_vals, large_vals, result1))
        
        # 2. ë§¤ìš° ì‘ì€ ê°’ë“¤ì˜ ë‚˜ëˆ—ì…ˆ
        small_vals = torch.tensor([1e-20, 1e-25, 1e-30], dtype=torch.float16)
        tiny_vals = torch.tensor([1e-35, 1e-38, 1e-40], dtype=torch.float16)
        result2 = small_vals / tiny_vals
        scenarios.append(('Small division', small_vals, tiny_vals, result2))
        
        # 3. ì§€ìˆ˜ í•¨ìˆ˜
        large_inputs = torch.tensor([50, 100, 200], dtype=torch.float16)
        result3 = torch.exp(large_inputs)
        scenarios.append(('Exponential overflow', large_inputs, None, result3))
        
        # 4. ë¡œê·¸ í•¨ìˆ˜ (ìŒìˆ˜ë‚˜ 0 ì…ë ¥)
        problematic_inputs = torch.tensor([0, -1, -10], dtype=torch.float16)
        result4 = torch.log(problematic_inputs)
        scenarios.append(('Log of non-positive', problematic_inputs, None, result4))
        
        # 5. Gradient explosion ì‹œë‚˜ë¦¬ì˜¤
        gradients = torch.tensor([1e5, 1e6, 1e7], dtype=torch.float16)
        learning_rate = torch.tensor(0.1, dtype=torch.float16)
        result5 = gradients * learning_rate
        scenarios.append(('Gradient explosion', gradients, learning_rate, result5))
        
        return scenarios
    
    def analyze_scenarios(self):
        """ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„"""
        scenarios = self.create_problematic_scenarios()
        
        print("NaN/Inf Generation Scenarios Analysis")
        print("=" * 60)
        
        for scenario_name, input1, input2, result in scenarios:
            print(f"\nScenario: {scenario_name}")
            print("-" * 40)
            
            print(f"Input 1: {input1}")
            if input2 is not None:
                print(f"Input 2: {input2}")
            print(f"Result: {result}")
            
            nan_count = torch.isnan(result).sum().item()
            inf_count = torch.isinf(result).sum().item()
            
            print(f"NaN count: {nan_count}")
            print(f"Inf count: {inf_count}")
            
            if nan_count > 0 or inf_count > 0:
                print("âš ï¸  PROBLEMATIC SCENARIO DETECTED!")
                
                # FP32ë¡œ ê°™ì€ ì—°ì‚° ìˆ˜í–‰í•´ì„œ ë¹„êµ
                if input2 is not None:
                    fp32_result = input1.float() * input2.float()  # ì˜ˆì‹œ ì—°ì‚°
                else:
                    fp32_result = torch.exp(input1.float()) if 'Exponential' in scenario_name else torch.log(input1.float())
                
                print(f"FP32 result: {fp32_result}")
                print(f"FP32 NaN count: {torch.isnan(fp32_result).sum().item()}")
                print(f"FP32 Inf count: {torch.isinf(fp32_result).sum().item()}")

def create_custom_float_format_simulator():
    """ì»¤ìŠ¤í…€ float í¬ë§· ì‹œë®¬ë ˆì´í„°"""
    
    class CustomFloatFormat:
        def __init__(self, sign_bits=1, exponent_bits=8, mantissa_bits=23):
            self.sign_bits = sign_bits
            self.exponent_bits = exponent_bits
            self.mantissa_bits = mantissa_bits
            self.total_bits = sign_bits + exponent_bits + mantissa_bits
            self.bias = (2 ** (exponent_bits - 1)) - 1
            
        def get_limits(self):
            """í¬ë§·ì˜ í•œê³„ê°’ ê³„ì‚°"""
            max_exponent = (2 ** self.exponent_bits) - 2  # ëª¨ë“  1ì€ íŠ¹ìˆ˜ê°’ìš©
            min_exponent = 1  # 0ì€ denormal/zeroìš©
            
            # ìµœëŒ€ê°’: 2^(max_exp - bias) * (2 - 2^(-mantissa_bits))
            max_val = (2 ** (max_exponent - self.bias)) * (2 - 2**(-self.mantissa_bits))
            
            # ìµœì†Œ ì •ê·œê°’: 2^(min_exp - bias)
            min_normal = 2 ** (min_exponent - self.bias)
            
            # ìµœì†Œ denormalê°’: 2^(1 - bias - mantissa_bits)
            min_denormal = 2 ** (1 - self.bias - self.mantissa_bits)
            
            return {
                'max': max_val,
                'min_normal': min_normal,
                'min_denormal': min_denormal,
                'exponent_range': (min_exponent - self.bias, max_exponent - self.bias)
            }
        
        def analyze_representation_density(self, value_range):
            """ì£¼ì–´ì§„ ë²”ìœ„ì—ì„œì˜ í‘œí˜„ ê°€ëŠ¥í•œ ê°’ì˜ ë°€ë„ ë¶„ì„"""
            # ì´ê²ƒì€ ì‹¤ì œ êµ¬í˜„ë³´ë‹¤ëŠ” ê°œë…ì  ë¶„ì„
            limits = self.get_limits()
            
            analysis = {
                'format': f"Sign:{self.sign_bits} Exp:{self.exponent_bits} Mantissa:{self.mantissa_bits}",
                'total_bits': self.total_bits,
                'limits': limits,
                'representation_count': 2 ** self.total_bits - 2,  # NaN, Inf ì œì™¸
            }
            
            return analysis
    
    # ë‹¤ì–‘í•œ í¬ë§· í…ŒìŠ¤íŠ¸
    formats = {
        'FP32': CustomFloatFormat(1, 8, 23),
        'FP16': CustomFloatFormat(1, 5, 10),
        'BF16': CustomFloatFormat(1, 8, 7),
        'Custom_High_Precision': CustomFloatFormat(1, 6, 25),  # ë†’ì€ ì •ë°€ë„
        'Custom_Large_Range': CustomFloatFormat(1, 10, 5),     # í° ë²”ìœ„
        'Custom_Balanced': CustomFloatFormat(1, 7, 16),        # ê· í˜•ì¡íŒ í¬ë§·
    }
    
    print("Custom Float Format Analysis")
    print("=" * 60)
    
    for format_name, format_obj in formats.items():
        analysis = format_obj.analyze_representation_density(None)
        limits = analysis['limits']
        
        print(f"\n{format_name}:")
        print(f"  Format: {analysis['format']}")
        print(f"  Total bits: {analysis['total_bits']}")
        print(f"  Max value: {limits['max']:.2e}")
        print(f"  Min normal: {limits['min_normal']:.2e}")
        print(f"  Min denormal: {limits['min_denormal']:.2e}")
        print(f"  Exponent range: {limits['exponent_range']}")
        print(f"  Representable values: {analysis['representation_count']:,}")

# ì‹¤í–‰ ë° ë°ëª¨ í•¨ìˆ˜ë“¤
def run_comprehensive_analysis():
    """í¬ê´„ì ì¸ ì •ë°€ë„ ë¶„ì„ ì‹¤í–‰"""
    print("Starting Comprehensive Float Precision Analysis...")
    
    # 1. ë¹„íŠ¸ ë ˆë²¨ ë¶„ì„
    analyzer = FloatBitAnalyzer()
    
    # í…ŒìŠ¤íŠ¸ í…ì„œ ìƒì„±
    test_tensor_fp32 = torch.randn(1000, dtype=torch.float32) * 1000
    test_tensor_fp16 = test_tensor_fp32.half()
    
    print("\n1. Bit-level Analysis")
    print("-" * 30)
    
    fp32_bits = analyzer.analyze_tensor_bits(test_tensor_fp32, 'fp32')
    fp16_bits = analyzer.analyze_tensor_bits(test_tensor_fp16, 'fp16')
    
    print("FP32 bit analysis:")
    for key, value in fp32_bits.items():
        if isinstance(value, dict):
            print(f"  {key}: {dict(list(value.items())[:5])}...")  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        else:
            print(f"  {key}: {value}")
    
    print("\nFP16 bit analysis:")
    for key, value in fp16_bits.items():
        if isinstance(value, dict):
            print(f"  {key}: {dict(list(value.items())[:5])}...")
        else:
            print(f"  {key}: {value}")
    
    # 2. ì •ë°€ë„ ë²”ìœ„ íƒêµ¬
    print("\n2. Precision Range Exploration")
    print("-" * 30)
    
    explorer = PrecisionRangeExplorer()
    conversion_df = explorer.test_precision_conversion()
    
    print(f"Tested {len(conversion_df)} conversions")
    print(f"Max conversion error: {conversion_df['conversion_error'].max():.2e}")
    print(f"Mean conversion error: {conversion_df['conversion_error'].mean():.2e}")
    
    # ì‹œê°í™”
    explorer.plot_precision_analysis(conversion_df)
    
    # 3. NaN ì›ì¸ ì¡°ì‚¬
    print("\n3. NaN Source Investigation")
    print("-" * 30)
    
    investigator = NaNInvestigator()
    investigator.analyze_scenarios()
    
    # 4. ì»¤ìŠ¤í…€ í¬ë§· ë¶„ì„
    print("\n4. Custom Float Format Analysis")
    print("-" * 30)
    
    create_custom_float_format_simulator()

# Colabì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ì‹œì‘ í•¨ìˆ˜
def start_monitoring():
    """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
    print("ğŸ” Float Precision Monitoring Started!")
    print("Use the following functions:")
    print("- run_comprehensive_analysis(): í¬ê´„ì ì¸ ë¶„ì„ ì‹¤í–‰")
    print("- MultiPrecisionTrainer(): ResNet í›ˆë ¨ ëª¨ë‹ˆí„°ë§")
    print("- FloatBitAnalyzer(): ë¹„íŠ¸ ë ˆë²¨ ë¶„ì„")
    print("- NaNInvestigator(): NaN ì›ì¸ ì¡°ì‚¬")

if __name__ == "__main__":
    start_monitoring()